# audio_enhancer.py
"""
ìŒì„± í’ˆì§ˆ í–¥ìƒ ëª¨ë“ˆ (Audio Enhancement Module)
ê²½ì§„ëŒ€íšŒìš© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ë…¸ì´ì¦ˆ ê°ì†Œ
- ìŠ¤í™íŠ¸ëŸ´ í•„í„°ë§
- ìŒì„± ëŒ€ì—­ ìµœì í™”
- ì‹œê°í™” ë° ì§€í‘œ ì¸¡ì •
"""

import numpy as np
import librosa
import librosa.display
import soundfile as sf
import matplotlib.pyplot as plt
from scipy import signal
from typing import Tuple, Dict, Optional
import os
from datetime import datetime
import platform

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'  # ìœˆë„ìš°
elif platform.system() == 'Darwin':
    plt.rcParams['font.family'] = 'AppleGothic'  # ë§¥
else:
    plt.rcParams['font.family'] = 'DejaVu Sans'  # ë¦¬ëˆ…ìŠ¤
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

class AudioEnhancer:
    """ìŒì„± í’ˆì§ˆ í–¥ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, target_sr: int = 16000):
        """
        Args:
            target_sr: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ETRI STT ê¶Œì¥: 16000)
        """
        self.target_sr = target_sr
        self.metrics = {}
        
    def enhance(self, audio_path: str, output_path: Optional[str] = None, 
                visualize: bool = True) -> Tuple[str, Dict]:
        """
        ìŒì„± í’ˆì§ˆ í–¥ìƒ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            audio_path: ì…ë ¥ ìŒì„± íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            visualize: ì‹œê°í™” ì—¬ë¶€
            
        Returns:
            (ì¶œë ¥ íŒŒì¼ ê²½ë¡œ, ê°œì„  ì§€í‘œ ë”•ì…”ë„ˆë¦¬)
        """
        print("="*50)
        print("ğŸµ ìŒì„± í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*50)
        
        # 1. ìŒì„± ë¡œë“œ
        print("ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ ì¤‘...")
        y_original, sr_original = librosa.load(audio_path, sr=None)
        y, sr = librosa.load(audio_path, sr=self.target_sr)
        print(f"  âœ“ ì›ë³¸ ìƒ˜í”Œë ˆì´íŠ¸: {sr_original}Hz â†’ {self.target_sr}Hz ë¦¬ìƒ˜í”Œë§")
        
        # ì›ë³¸ ì§€í‘œ ì¸¡ì •
        metrics_before = self._calculate_metrics(y, sr, "ì›ë³¸")
        
        # 2. ë…¸ì´ì¦ˆ í”„ë¡œíŒŒì¼ ë¶„ì„
        print("\nğŸ” ë…¸ì´ì¦ˆ í”„ë¡œíŒŒì¼ ë¶„ì„ ì¤‘...")
        noise_profile = self._analyze_noise_profile(y, sr)
        print(f"  âœ“ ë…¸ì´ì¦ˆ ë ˆë²¨: {noise_profile['noise_level']:.2f} dB")
        print(f"  âœ“ SNR: {noise_profile['snr']:.2f} dB")
        
        # 3. ìŠ¤í™íŠ¸ëŸ´ ë…¸ì´ì¦ˆ ê²Œì´íŒ…
        print("\nğŸšï¸ ìŠ¤í™íŠ¸ëŸ´ ë…¸ì´ì¦ˆ ê²Œì´íŒ… ì ìš© ì¤‘...")
        y_denoised = self._spectral_gating(y, sr, noise_profile)
        print("  âœ“ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ")
        
        # 4. ìŒì„± ëŒ€ì—­ ê°•ì¡° (300-3400Hz)
        print("\nğŸ“» ìŒì„± ì£¼íŒŒìˆ˜ ëŒ€ì—­ ìµœì í™” ì¤‘...")
        y_enhanced = self._enhance_speech_band(y_denoised, sr)
        print("  âœ“ ìŒì„± ëª…ë£Œë„ í–¥ìƒ í•„í„° ì ìš©")
        
        # 5. ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì •ê·œí™”
        print("\nğŸ“Š ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì •ê·œí™” ì¤‘...")
        y_normalized = self._normalize_dynamic_range(y_enhanced)
        print("  âœ“ ìŒëŸ‰ ê· ì¼í™” ì™„ë£Œ")
        
        # ì²˜ë¦¬ í›„ ì§€í‘œ ì¸¡ì •
        metrics_after = self._calculate_metrics(y_normalized, sr, "ì²˜ë¦¬ í›„")
        
        # ê°œì„ ìœ¨ ê³„ì‚° ë° metrics ì €ì¥ (ì¤‘ìš”: ì‹œê°í™” ì „ì—!)
        improvement = self._calculate_improvement(metrics_before, metrics_after)
        self.metrics = {
            'before': metrics_before,
            'after': metrics_after,
            'improvement': improvement
        }
        
        # 6. ì €ì¥
        if output_path is None:
            basename = os.path.splitext(os.path.basename(audio_path))[0]
            output_path = f"{basename}_enhanced.wav"
        
        sf.write(output_path, y_normalized, sr)
        print(f"\nğŸ’¾ í–¥ìƒëœ ìŒì„± ì €ì¥: {output_path}")
        
        # 7. ì‹œê°í™” (metricsê°€ ì„¤ì •ëœ í›„!)
        if visualize:
            self._visualize_enhancement(
                y, y_normalized, sr, 
                metrics_before, metrics_after,
                audio_path
            )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“ˆ ìŒì„± í’ˆì§ˆ ê°œì„  ê²°ê³¼")
        print("="*50)
        print(f"  ğŸ”¹ ë…¸ì´ì¦ˆ ê°ì†Œ: {improvement['noise_reduction']:.1f}%")
        print(f"  ğŸ”¹ SNR ê°œì„ : {improvement['snr_improvement']:.1f}%")
        print(f"  ğŸ”¹ ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ìµœì í™”: {improvement['dynamic_range']:.1f}%")
        print(f"  ğŸ”¹ ì „ì²´ í’ˆì§ˆ í–¥ìƒ: {improvement['overall']:.1f}%")
        print("="*50)
        
        return output_path, self.metrics
    
    def _analyze_noise_profile(self, y: np.ndarray, sr: int) -> Dict:
        """ë…¸ì´ì¦ˆ í”„ë¡œíŒŒì¼ ë¶„ì„"""
        # ì²« 0.5ì´ˆë¥¼ ë…¸ì´ì¦ˆ êµ¬ê°„ìœ¼ë¡œ ê°€ì • (ìƒë‹´ ì‹œì‘ ì „ ì¹¨ë¬µ)
        noise_sample = y[:int(sr * 0.5)]
        
        # ë…¸ì´ì¦ˆ ë ˆë²¨ ê³„ì‚° (RMS)
        noise_level = 20 * np.log10(np.sqrt(np.mean(noise_sample**2)) + 1e-10)
        
        # ì „ì²´ ì‹ í˜¸ ë ˆë²¨
        signal_level = 20 * np.log10(np.sqrt(np.mean(y**2)) + 1e-10)
        
        # SNR ê³„ì‚°
        snr = signal_level - noise_level
        
        # ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
        noise_fft = np.abs(np.fft.rfft(noise_sample))
        noise_freq = np.fft.rfftfreq(len(noise_sample), 1/sr)
        
        return {
            'noise_level': noise_level,
            'signal_level': signal_level,
            'snr': snr,
            'noise_spectrum': noise_fft,
            'frequencies': noise_freq
        }
    
    def _spectral_gating(self, y: np.ndarray, sr: int, 
                        noise_profile: Dict) -> np.ndarray:
        """ìŠ¤í™íŠ¸ëŸ´ ê²Œì´íŒ…ì„ í†µí•œ ë…¸ì´ì¦ˆ ì œê±°"""
        # STFT ë³€í™˜
        D = librosa.stft(y)
        magnitude = np.abs(D)
        phase = np.angle(D)
        
        # ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ ì„ê³„ê°’ (ë…¸ì´ì¦ˆ ë ˆë²¨ì˜ 1.5ë°°)
        threshold = np.percentile(magnitude, 20)
        
        # ê²Œì´íŒ… ì ìš© (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
        mask = magnitude > threshold
        magnitude_gated = magnitude * mask
        
        # ìŠ¤ë¬´ë”© (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
        from scipy.ndimage import gaussian_filter
        mask_smooth = gaussian_filter(mask.astype(float), sigma=1.0)
        magnitude_gated = magnitude * mask_smooth
        
        # ì—­ë³€í™˜
        D_gated = magnitude_gated * np.exp(1j * phase)
        y_gated = librosa.istft(D_gated)
        
        return y_gated
    
    def _enhance_speech_band(self, y: np.ndarray, sr: int) -> np.ndarray:
        """ìŒì„± ì£¼íŒŒìˆ˜ ëŒ€ì—­ ê°•ì¡° (300-3400Hz)"""
        # ëŒ€ì—­í†µê³¼ í•„í„° ì„¤ê³„
        nyquist = sr / 2
        low_freq = 300 / nyquist
        high_freq = min(3400 / nyquist, 0.99)  # Nyquist í•œê³„ ì²´í¬
        
        # Butterworth í•„í„°
        sos = signal.butter(
            N=5,
            Wn=[low_freq, high_freq],
            btype='band',
            output='sos'
        )
        
        # í•„í„° ì ìš©
        y_filtered = signal.sosfilt(sos, y)
        
        # ì›ë³¸ê³¼ ë¯¹ì‹± (ê³¼ë„í•œ í•„í„°ë§ ë°©ì§€)
        alpha = 0.7  # í•„í„°ë§ ê°•ë„
        y_enhanced = alpha * y_filtered + (1 - alpha) * y
        
        return y_enhanced
    
    def _normalize_dynamic_range(self, y: np.ndarray) -> np.ndarray:
        """ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ì •ê·œí™”"""
        # RMS ì •ê·œí™”
        rms = np.sqrt(np.mean(y**2))
        target_rms = 0.1  # ëª©í‘œ RMS ë ˆë²¨
        
        if rms > 0:
            y_normalized = y * (target_rms / rms)
        else:
            y_normalized = y
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(y_normalized))
        if max_val > 0.95:
            y_normalized = y_normalized * (0.95 / max_val)
        
        # ë¶€ë“œëŸ¬ìš´ ì••ì¶• (ì»´í”„ë ˆì„œ)
        threshold = 0.5
        ratio = 4.0  # 4:1 ì••ì¶•
        
        mask = np.abs(y_normalized) > threshold
        y_compressed = y_normalized.copy()
        y_compressed[mask] = np.sign(y_normalized[mask]) * (
            threshold + (np.abs(y_normalized[mask]) - threshold) / ratio
        )
        
        return y_compressed
    
    def _calculate_metrics(self, y: np.ndarray, sr: int, label: str) -> Dict:
        """ìŒì„± í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        # RMS (Root Mean Square) - í‰ê·  ìŒëŸ‰
        rms = np.sqrt(np.mean(y**2))
        rms_db = 20 * np.log10(rms + 1e-10)
        
        # ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€
        max_val = np.max(np.abs(y))
        min_val = np.mean(np.abs(y[np.abs(y) > np.percentile(np.abs(y), 10)]))
        dynamic_range = 20 * np.log10((max_val / (min_val + 1e-10)) + 1e-10)
        
        # ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ (ìŒìƒ‰ ë°ê¸°)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        avg_centroid = np.mean(spectral_centroids)
        
        # ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸ (ë…¸ì´ì¦ˆ ì§€í‘œ)
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        avg_zcr = np.mean(zcr)
        
        print(f"\nğŸ“Š {label} ìŒì„± ì§€í‘œ:")
        print(f"  â€¢ RMS ë ˆë²¨: {rms_db:.2f} dB")
        print(f"  â€¢ ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€: {dynamic_range:.2f} dB")
        print(f"  â€¢ ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ: {avg_centroid:.0f} Hz")
        print(f"  â€¢ ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸: {avg_zcr:.4f}")
        
        return {
            'rms': rms,
            'rms_db': rms_db,
            'dynamic_range': dynamic_range,
            'spectral_centroid': avg_centroid,
            'zcr': avg_zcr
        }
    
    def _calculate_improvement(self, before: Dict, after: Dict) -> Dict:
        """ê°œì„ ìœ¨ ê³„ì‚°"""
        # ë…¸ì´ì¦ˆ ê°ì†Œ (ZCR ê°ì†Œìœ¨)
        noise_reduction = (1 - after['zcr'] / before['zcr']) * 100
        
        # SNR ê°œì„  (RMS ì¦ê°€ìœ¨)
        snr_improvement = ((after['rms_db'] - before['rms_db']) / abs(before['rms_db'])) * 100
        
        # ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ìµœì í™”
        target_dynamic_range = 40  # ëª©í‘œ ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ (dB)
        before_diff = abs(before['dynamic_range'] - target_dynamic_range)
        after_diff = abs(after['dynamic_range'] - target_dynamic_range)
        dynamic_improvement = (1 - after_diff / before_diff) * 100 if before_diff > 0 else 0
        
        # ì „ì²´ í’ˆì§ˆ í–¥ìƒ (ê°€ì¤‘ í‰ê· )
        overall = (noise_reduction * 0.4 + abs(snr_improvement) * 0.3 + dynamic_improvement * 0.3)
        
        return {
            'noise_reduction': max(0, noise_reduction),
            'snr_improvement': snr_improvement,
            'dynamic_range': dynamic_improvement,
            'overall': overall
        }
    
    def _visualize_enhancement(self, y_before: np.ndarray, y_after: np.ndarray,
                              sr: int, metrics_before: Dict, metrics_after: Dict,
                              audio_path: str):
        """ì²˜ë¦¬ ì „í›„ ì‹œê°í™”"""
        fig, axes = plt.subplots(3, 2, figsize=(15, 12))
        fig.suptitle('ìŒì„± í’ˆì§ˆ í–¥ìƒ ê²°ê³¼ (Audio Enhancement Results)', fontsize=16, fontweight='bold')
        
        # 1. íŒŒí˜• (Waveform)
        time_before = np.linspace(0, len(y_before)/sr, len(y_before))
        time_after = np.linspace(0, len(y_after)/sr, len(y_after))
        
        axes[0, 0].plot(time_before, y_before, alpha=0.7)
        axes[0, 0].set_title('ì›ë³¸ íŒŒí˜• (Original)', fontweight='bold')
        axes[0, 0].set_xlabel('Time (s)')
        axes[0, 0].set_ylabel('Amplitude')
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].plot(time_after, y_after, alpha=0.7, color='green')
        axes[0, 1].set_title('í–¥ìƒëœ íŒŒí˜• (Enhanced)', fontweight='bold')
        axes[0, 1].set_xlabel('Time (s)')
        axes[0, 1].set_ylabel('Amplitude')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 2. ìŠ¤í™íŠ¸ë¡œê·¸ë¨ (Spectrogram)
        D_before = librosa.amplitude_to_db(np.abs(librosa.stft(y_before)), ref=np.max)
        D_after = librosa.amplitude_to_db(np.abs(librosa.stft(y_after)), ref=np.max)
        
        img1 = librosa.display.specshow(D_before, sr=sr, x_axis='time', y_axis='hz', ax=axes[1, 0])
        axes[1, 0].set_title('ì›ë³¸ ìŠ¤í™íŠ¸ë¡œê·¸ë¨', fontweight='bold')
        axes[1, 0].set_ylim(0, 4000)  # ìŒì„± ì£¼íŒŒìˆ˜ ëŒ€ì—­ í¬ì»¤ìŠ¤
        plt.colorbar(img1, ax=axes[1, 0], format='%+2.0f dB')
        
        img2 = librosa.display.specshow(D_after, sr=sr, x_axis='time', y_axis='hz', ax=axes[1, 1])
        axes[1, 1].set_title('í–¥ìƒëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨', fontweight='bold')
        axes[1, 1].set_ylim(0, 4000)
        plt.colorbar(img2, ax=axes[1, 1], format='%+2.0f dB')
        
        # 3. í’ˆì§ˆ ì§€í‘œ ë¹„êµ (Metrics Comparison)
        metrics_labels = ['RMS (dB)', 'Dynamic\nRange (dB)', 'Spectral\nCentroid (Hz)', 'ZCR']
        before_values = [
            metrics_before['rms_db'],
            metrics_before['dynamic_range'],
            metrics_before['spectral_centroid']/100,  # ìŠ¤ì¼€ì¼ ì¡°ì •
            metrics_before['zcr']*100  # ìŠ¤ì¼€ì¼ ì¡°ì •
        ]
        after_values = [
            metrics_after['rms_db'],
            metrics_after['dynamic_range'],
            metrics_after['spectral_centroid']/100,
            metrics_after['zcr']*100
        ]
        
        x = np.arange(len(metrics_labels))
        width = 0.35
        
        axes[2, 0].bar(x - width/2, before_values, width, label='ì›ë³¸', alpha=0.8, color='blue')
        axes[2, 0].bar(x + width/2, after_values, width, label='í–¥ìƒ', alpha=0.8, color='green')
        axes[2, 0].set_xlabel('ì§€í‘œ (Metrics)')
        axes[2, 0].set_ylabel('ê°’ (Value)')
        axes[2, 0].set_title('í’ˆì§ˆ ì§€í‘œ ë¹„êµ', fontweight='bold')
        axes[2, 0].set_xticks(x)
        axes[2, 0].set_xticklabels(metrics_labels)
        axes[2, 0].legend()
        axes[2, 0].grid(True, alpha=0.3)
        
        # 4. ê°œì„ ìœ¨ í‘œì‹œ (Improvement Rates)
        improvement = self.metrics['improvement']
        categories = ['ë…¸ì´ì¦ˆ ê°ì†Œ\n(Noise Reduction)', 
                     'SNR ê°œì„ \n(SNR Improvement)', 
                     'ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€\n(Dynamic Range)',
                     'ì „ì²´ í’ˆì§ˆ\n(Overall Quality)']
        improvements = [
            improvement['noise_reduction'],
            abs(improvement['snr_improvement']),
            improvement['dynamic_range'],
            improvement['overall']
        ]
        
        colors = ['red', 'orange', 'yellow', 'green']
        bars = axes[2, 1].bar(categories, improvements, color=colors, alpha=0.7)
        axes[2, 1].set_ylabel('ê°œì„ ìœ¨ (%)')
        axes[2, 1].set_title('í’ˆì§ˆ ê°œì„  ì§€í‘œ', fontweight='bold')
        axes[2, 1].set_ylim(0, max(improvements) * 1.2)
        axes[2, 1].grid(True, alpha=0.3)
        
        # ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
        for bar, value in zip(bars, improvements):
            height = bar.get_height()
            axes[2, 1].text(bar.get_x() + bar.get_width()/2., height,
                          f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # ì €ì¥
        output_filename = f"enhancement_visualization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(output_filename, dpi=150, bbox_inches='tight')
        print(f"\nğŸ“Š ì‹œê°í™” ì €ì¥: {output_filename}")
        plt.show()


# ê°„í¸ ì‚¬ìš© í•¨ìˆ˜
def enhance_audio(input_path: str, output_path: Optional[str] = None, 
                 visualize: bool = True) -> Tuple[str, Dict]:
    """
    ìŒì„± í’ˆì§ˆ í–¥ìƒ ê°„í¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        input_path: ì…ë ¥ ìŒì„± íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
        visualize: ì‹œê°í™” ì—¬ë¶€
        
    Returns:
        (í–¥ìƒëœ ìŒì„± íŒŒì¼ ê²½ë¡œ, ê°œì„  ì§€í‘œ)
    """
    enhancer = AudioEnhancer(target_sr=16000)
    return enhancer.enhance(input_path, output_path, visualize)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        print(f"ğŸµ ìŒì„± íŒŒì¼ ì²˜ë¦¬: {input_file}")
        output_file, metrics = enhance_audio(input_file)
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    else:
        print("ì‚¬ìš©ë²•: python audio_enhancer.py [ì˜¤ë””ì˜¤íŒŒì¼ê²½ë¡œ]")
        print("ì˜ˆì‹œ: python audio_enhancer.py test.mp3")