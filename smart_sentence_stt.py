# smart_sentence_stt.py
"""
ë¬¸ì¥ ë‹¨ìœ„ ìŠ¤ë§ˆíŠ¸ STT ì²˜ë¦¬ê¸°
ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ìœ„í•œ ë¬¸ì¥ë³„ ì²­í‚¹
"""

import json
import base64
import urllib3
import time
import tempfile
import os
import numpy as np
from typing import Optional, Dict, List, Callable, Tuple
from pathlib import Path
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# pydub ì„¤ì •
try:
    from pydub import AudioSegment
    from pydub.silence import detect_nonsilent, split_on_silence
    import imageio_ffmpeg
    
    ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()
    AudioSegment.converter = ffmpeg_path
    AudioSegment.ffmpeg = ffmpeg_path
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False
    print("âš ï¸ pydub ì„¤ì¹˜ í•„ìš”")


@dataclass
class SentenceChunk:
    """ë¬¸ì¥ ì²­í¬ ì •ë³´"""
    index: int
    audio_path: str
    start_time: float  # ì´ˆ ë‹¨ìœ„
    end_time: float
    duration: float
    estimated_text_length: int  # ì˜ˆìƒ í…ìŠ¤íŠ¸ ê¸¸ì´
    is_merged: bool = False  # ë‹¤ë¥¸ ì²­í¬ì™€ ë³‘í•©ë¨
    merged_indices: List[int] = None  # ë³‘í•©ëœ ì²­í¬ ì¸ë±ìŠ¤ë“¤


@dataclass
class SentenceResult:
    """ë¬¸ì¥ ì²˜ë¦¬ ê²°ê³¼"""
    chunk_index: int
    text: str
    start_time: float
    end_time: float
    success: bool
    error: Optional[str] = None


class SmartSentenceSTT:
    """ë¬¸ì¥ ë‹¨ìœ„ ìŠ¤ë§ˆíŠ¸ STT ì²˜ë¦¬ê¸°"""
    
    API_URL = "http://epretx.etri.re.kr:8000/api/WiseASR_Recognition"
    
    # ì²­í¬ ì„¤ì •
    MIN_CHUNK_SEC = 3.0   # ìµœì†Œ 3ì´ˆ
    MAX_CHUNK_SEC = 15.0  # ìµœëŒ€ 15ì´ˆ (API ì•ˆì „ ë§ˆì§„)
    OPTIMAL_CHUNK_SEC = 8.0  # ìµœì  8ì´ˆ
    
    # ë¬´ìŒ ê°ì§€ ì„¤ì •
    SILENCE_THRESH = -35  # dB
    MIN_SILENCE_LEN = 300  # ms (ë¬¸ì¥ ê²½ê³„)
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.http = urllib3.PoolManager()
        self.sentence_boundaries = []  # ë¬¸ì¥ ê²½ê³„ ì‹œê°„ ì €ì¥
    
    def process_with_timestamps(self,
                               audio_path: str,
                               language: str = "korean",
                               max_workers: int = 2,
                               progress_callback: Optional[Callable] = None) -> Dict:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ STT ì²˜ë¦¬
        
        Returns:
            {
                'success': bool,
                'sentences': [
                    {
                        'text': str,
                        'start_time': float,
                        'end_time': float
                    }
                ],
                'formatted_text': str  # [MM:SS] í˜•ì‹ í¬í•¨
            }
        """
        start_time = time.time()
        
        # 1. ë¬¸ì¥ ë‹¨ìœ„ ì²­í¬ ìƒì„±
        if progress_callback:
            progress_callback(0, 100, "ìŒì„± ë¶„ì„ ë° ë¬¸ì¥ ê²½ê³„ ê°ì§€ ì¤‘...", None)
        
        chunks = self._create_sentence_chunks(audio_path)
        
        if not chunks:
            return {
                'success': False,
                'sentences': [],
                'formatted_text': '',
                'error': 'ë¬¸ì¥ ì²­í¬ ìƒì„± ì‹¤íŒ¨'
            }
        
        print(f"ğŸ“¦ {len(chunks)}ê°œ ë¬¸ì¥ ì²­í¬ ìƒì„±ë¨")
        
        # 2. ì²­í¬ ì²˜ë¦¬ (ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨)
        if len(chunks) <= 3 or max_workers == 1:
            results = self._process_sequential(chunks, language, progress_callback)
        else:
            results = self._process_parallel(chunks, language, max_workers, progress_callback)
        
        # 3. ê²°ê³¼ ì •ë¦¬ ë° í¬ë§·íŒ…
        sentences = self._organize_results(results, chunks)
        formatted_text = self._format_with_timestamps(sentences)
        
        # í†µê³„
        success_count = sum(1 for r in results if r.success)
        total_time = time.time() - start_time
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for chunk in chunks:
            try:
                os.remove(chunk.audio_path)
            except:
                pass
        
        return {
            'success': success_count > len(chunks) * 0.5,
            'sentences': sentences,
            'formatted_text': formatted_text,
            'stats': {
                'total_chunks': len(chunks),
                'success_chunks': success_count,
                'processing_time': total_time
            }
        }
    
    def _create_sentence_chunks(self, audio_path: str) -> List[SentenceChunk]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ ë¶„í• """
        if not PYDUB_AVAILABLE:
            return []
        
        audio = AudioSegment.from_file(audio_path)
        total_duration_ms = len(audio)
        
        # 1. ë¬´ìŒ ê¸°ë°˜ ë¶„í• 
        chunks_raw = split_on_silence(
            audio,
            min_silence_len=self.MIN_SILENCE_LEN,
            silence_thresh=self.SILENCE_THRESH,
            keep_silence=200  # ì•ë’¤ 200ms ìœ ì§€ (ìì—°ìŠ¤ëŸ¬ìš´ ë°œí™”)
        )
        
        if not chunks_raw:
            # ë¬´ìŒì´ ì—†ìœ¼ë©´ ê³ ì • ê¸¸ì´ë¡œ ë¶„í• 
            return self._fallback_fixed_chunks(audio)
        
        # 2. ì²­í¬ ì •ë³´ ìƒì„±
        temp_dir = tempfile.mkdtemp()
        chunks = []
        current_position = 0
        
        for i, chunk_audio in enumerate(chunks_raw):
            chunk_duration_ms = len(chunk_audio)
            
            # ì‹œì‘/ë ì‹œê°„ ê³„ì‚°
            # ì‹¤ì œ ì˜¤ë””ì˜¤ì—ì„œì˜ ìœ„ì¹˜ ì°¾ê¸° (ê·¼ì‚¬ì¹˜)
            start_ms = current_position
            end_ms = start_ms + chunk_duration_ms
            
            # ì²­í¬ ì •ë³´ ìƒì„±
            chunk_info = SentenceChunk(
                index=i,
                audio_path="",  # ë‚˜ì¤‘ì— ì„¤ì •
                start_time=start_ms / 1000.0,
                end_time=end_ms / 1000.0,
                duration=chunk_duration_ms / 1000.0,
                estimated_text_length=self._estimate_text_length(chunk_duration_ms)
            )
            
            chunks.append((chunk_info, chunk_audio))
            current_position = end_ms
        
        # 3. ì²­í¬ ìµœì í™” (ë³‘í•©/ë¶„í• )
        optimized_chunks = self._optimize_chunks(chunks)
        
        # 4. ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
        final_chunks = []
        for chunk_info, chunk_audio in optimized_chunks:
            chunk_path = os.path.join(temp_dir, f"sentence_{chunk_info.index:04d}.wav")
            chunk_audio.export(chunk_path, format="wav")
            chunk_info.audio_path = chunk_path
            final_chunks.append(chunk_info)
        
        return final_chunks
    
    def _optimize_chunks(self, 
                        chunks: List[Tuple[SentenceChunk, AudioSegment]]) -> List[Tuple[SentenceChunk, AudioSegment]]:
        """ì²­í¬ ìµœì í™” (ë³‘í•©/ë¶„í• )"""
        optimized = []
        i = 0
        
        while i < len(chunks):
            chunk_info, chunk_audio = chunks[i]
            
            # ë„ˆë¬´ ì§§ì€ ì²­í¬: ë‹¤ìŒê³¼ ë³‘í•©
            if chunk_info.duration < self.MIN_CHUNK_SEC and i < len(chunks) - 1:
                # ë‹¤ìŒ ì²­í¬ë“¤ê³¼ ë³‘í•©
                merged_audio = chunk_audio
                merged_info = SentenceChunk(
                    index=chunk_info.index,
                    audio_path="",
                    start_time=chunk_info.start_time,
                    end_time=chunk_info.end_time,
                    duration=chunk_info.duration,
                    estimated_text_length=chunk_info.estimated_text_length,
                    is_merged=True,
                    merged_indices=[chunk_info.index]
                )
                
                j = i + 1
                while j < len(chunks) and merged_info.duration < self.OPTIMAL_CHUNK_SEC:
                    next_info, next_audio = chunks[j]
                    
                    # ë³‘í•© í›„ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì¤‘ë‹¨
                    if merged_info.duration + next_info.duration > self.MAX_CHUNK_SEC:
                        break
                    
                    merged_audio += next_audio
                    merged_info.end_time = next_info.end_time
                    merged_info.duration = merged_info.end_time - merged_info.start_time
                    merged_info.estimated_text_length += next_info.estimated_text_length
                    merged_info.merged_indices.append(next_info.index)
                    j += 1
                
                optimized.append((merged_info, merged_audio))
                i = j
            
            # ë„ˆë¬´ ê¸´ ì²­í¬: ë¶„í• 
            elif chunk_info.duration > self.MAX_CHUNK_SEC:
                # ì¤‘ê°„ ì§€ì ì—ì„œ ë¶„í• 
                split_point = len(chunk_audio) // 2
                
                first_audio = chunk_audio[:split_point]
                second_audio = chunk_audio[split_point:]
                
                first_info = SentenceChunk(
                    index=chunk_info.index,
                    audio_path="",
                    start_time=chunk_info.start_time,
                    end_time=chunk_info.start_time + len(first_audio) / 1000.0,
                    duration=len(first_audio) / 1000.0,
                    estimated_text_length=chunk_info.estimated_text_length // 2
                )
                
                second_info = SentenceChunk(
                    index=chunk_info.index + 0.5,  # ì†Œìˆ˜ì ìœ¼ë¡œ êµ¬ë¶„
                    audio_path="",
                    start_time=first_info.end_time,
                    end_time=chunk_info.end_time,
                    duration=len(second_audio) / 1000.0,
                    estimated_text_length=chunk_info.estimated_text_length // 2
                )
                
                optimized.append((first_info, first_audio))
                optimized.append((second_info, second_audio))
                i += 1
            
            # ì ì ˆí•œ ê¸¸ì´: ê·¸ëŒ€ë¡œ ì‚¬ìš©
            else:
                optimized.append((chunk_info, chunk_audio))
                i += 1
        
        print(f"ğŸ“Š ì²­í¬ ìµœì í™”: {len(chunks)}ê°œ â†’ {len(optimized)}ê°œ")
        return optimized
    
    def _fallback_fixed_chunks(self, audio: AudioSegment) -> List[SentenceChunk]:
        """ë¬´ìŒ ê°ì§€ ì‹¤íŒ¨ ì‹œ ê³ ì • ê¸¸ì´ ë¶„í• """
        chunks = []
        temp_dir = tempfile.mkdtemp()
        
        chunk_ms = int(self.OPTIMAL_CHUNK_SEC * 1000)
        total_ms = len(audio)
        
        for i in range(0, total_ms, chunk_ms):
            start_ms = i
            end_ms = min(i + chunk_ms, total_ms)
            
            chunk_audio = audio[start_ms:end_ms]
            chunk_path = os.path.join(temp_dir, f"fixed_{i//chunk_ms:04d}.wav")
            chunk_audio.export(chunk_path, format="wav")
            
            chunks.append(SentenceChunk(
                index=i // chunk_ms,
                audio_path=chunk_path,
                start_time=start_ms / 1000.0,
                end_time=end_ms / 1000.0,
                duration=(end_ms - start_ms) / 1000.0,
                estimated_text_length=100  # ì¶”ì •ê°’
            ))
        
        return chunks
    
    def _estimate_text_length(self, duration_ms: int) -> int:
        """ìŒì„± ê¸¸ì´ë¡œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶”ì •"""
        # í•œêµ­ì–´ í‰ê· : ë¶„ë‹¹ 200ì ì •ë„
        chars_per_second = 200 / 60
        return int((duration_ms / 1000) * chars_per_second)
    
    def _api_call(self, chunk_path: str, language: str) -> SentenceResult:
        """API í˜¸ì¶œ"""
        try:
            with open(chunk_path, "rb") as f:
                audio_contents = base64.b64encode(f.read()).decode("utf8")
            
            request_json = {
                "argument": {
                    "language_code": language,
                    "audio": audio_contents
                }
            }
            
            response = self.http.request(
                "POST",
                self.API_URL,
                headers={
                    "Content-Type": "application/json; charset=UTF-8",
                    "Authorization": self.api_key
                },
                body=json.dumps(request_json),
                timeout=30.0
            )
            
            if response.status == 200:
                result = json.loads(response.data.decode("utf-8"))
                if result.get("result", -1) == 0:
                    text = result.get("return_object", {}).get("recognized", "")
                    return SentenceResult(
                        chunk_index=0,
                        text=text,
                        start_time=0,
                        end_time=0,
                        success=True
                    )
            
            return SentenceResult(
                chunk_index=0,
                text="",
                start_time=0,
                end_time=0,
                success=False,
                error=f"API Error: {response.status}"
            )
            
        except Exception as e:
            return SentenceResult(
                chunk_index=0,
                text="",
                start_time=0,
                end_time=0,
                success=False,
                error=str(e)
            )
    
    def _process_sequential(self, chunks: List[SentenceChunk], language: str,
                          progress_callback: Optional[Callable]) -> List[SentenceResult]:
        """ìˆœì°¨ ì²˜ë¦¬"""
        results = []
        total = len(chunks)
        
        for i, chunk in enumerate(chunks):
            if progress_callback:
                percent = int((i / total) * 100)
                progress_callback(percent, 100, f"ë¬¸ì¥ {i+1}/{total} ì²˜ë¦¬ ì¤‘...", None)
            
            result = self._api_call(chunk.audio_path, language)
            result.chunk_index = chunk.index
            result.start_time = chunk.start_time
            result.end_time = chunk.end_time
            results.append(result)
            
            if i < total - 1:
                time.sleep(1.0)
        
        return results
    
    def _process_parallel(self, chunks: List[SentenceChunk], language: str,
                        max_workers: int, progress_callback: Optional[Callable]) -> List[SentenceResult]:
        """ë³‘ë ¬ ì²˜ë¦¬"""
        results = []
        completed = 0
        total = len(chunks)
        
        with ThreadPoolExecutor(max_workers=min(max_workers, 2)) as executor:
            future_to_chunk = {}
            
            for i, chunk in enumerate(chunks):
                if i > 0 and i % 2 == 0:
                    time.sleep(0.5)  # API ë¶€í•˜ ê´€ë¦¬
                
                future = executor.submit(self._api_call, chunk.audio_path, language)
                future_to_chunk[future] = chunk
            
            for future in as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                
                try:
                    result = future.result(timeout=30)
                    result.chunk_index = chunk.index
                    result.start_time = chunk.start_time
                    result.end_time = chunk.end_time
                    results.append(result)
                    
                except Exception as e:
                    results.append(SentenceResult(
                        chunk_index=chunk.index,
                        text="",
                        start_time=chunk.start_time,
                        end_time=chunk.end_time,
                        success=False,
                        error=str(e)
                    ))
                
                completed += 1
                if progress_callback:
                    percent = int((completed / total) * 100)
                    progress_callback(percent, 100, f"ë³‘ë ¬ ì²˜ë¦¬ ì¤‘... {completed}/{total}", None)
        
        # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬
        results.sort(key=lambda x: x.start_time)
        return results
    
    def _organize_results(self, results: List[SentenceResult], 
                         chunks: List[SentenceChunk]) -> List[Dict]:
        """ê²°ê³¼ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì •ë¦¬"""
        sentences = []
        
        for result in results:
            if not result.success or not result.text:
                continue
            
            # ë³‘í•©ëœ ì²­í¬ ì²˜ë¦¬
            chunk = next((c for c in chunks if c.index == result.chunk_index), None)
            if chunk and chunk.is_merged and chunk.merged_indices:
                # ë³‘í•©ëœ ë¬¸ì¥ë“¤ì„ ë¶„ë¦¬
                text_sentences = self._split_text_to_sentences(result.text)
                
                if len(text_sentences) == 1:
                    # ë¶„ë¦¬ ì•ˆ ë¨: ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ
                    sentences.append({
                        'text': result.text.strip(),
                        'start_time': result.start_time,
                        'end_time': result.end_time
                    })
                else:
                    # ì‹œê°„ì„ ë¹„ë¡€ ë°°ë¶„
                    total_duration = result.end_time - result.start_time
                    current_time = result.start_time
                    
                    for sent_text in text_sentences:
                        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„ë¡€ë¡œ ì‹œê°„ í• ë‹¹
                        sent_duration = (len(sent_text) / len(result.text)) * total_duration
                        
                        sentences.append({
                            'text': sent_text.strip(),
                            'start_time': current_time,
                            'end_time': current_time + sent_duration
                        })
                        
                        current_time += sent_duration
            else:
                # ë‹¨ì¼ ë¬¸ì¥
                sentences.append({
                    'text': result.text.strip(),
                    'start_time': result.start_time,
                    'end_time': result.end_time
                })
        
        return sentences
    
    def _split_text_to_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬"""
        # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬ (í–¥í›„ ê°œì„  ê°€ëŠ¥)
        import re
        
        # ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
        sentences = re.split(r'([.!?]+)\s*', text)
        
        # ì¢…ê²° ë¶€í˜¸ë¥¼ ë¬¸ì¥ì— ë‹¤ì‹œ ë¶™ì´ê¸°
        result = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                result.append(sentences[i] + sentences[i+1])
            else:
                result.append(sentences[i])
        
        # ë¹ˆ ë¬¸ì¥ ì œê±°
        result = [s.strip() for s in result if s.strip()]
        
        return result if result else [text]
    
    def _format_with_timestamps(self, sentences: List[Dict]) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í¬ë§·íŒ…"""
        formatted_lines = []
        
        for sentence in sentences:
            # ì‹œê°„ í¬ë§· (MM:SS)
            minutes = int(sentence['start_time'] // 60)
            seconds = int(sentence['start_time'] % 60)
            timestamp = f"[{minutes:02d}:{seconds:02d}]"
            
            # ë¬¸ì¥ ì¶”ê°€
            formatted_lines.append(f"{timestamp} {sentence['text']}")
            formatted_lines.append("")  # ë¹ˆ ì¤„ ì¶”ê°€ (ê°€ë…ì„±)
        
        return "\n".join(formatted_lines)


# ê°„í¸ ì‚¬ìš© í•¨ìˆ˜
def transcribe_with_timestamps(audio_path: str, api_key: str,
                              max_workers: int = 2,
                              progress_callback: Optional[Callable] = None) -> Dict:
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ìŒì„± ì¸ì‹
    
    Returns:
        {
            'success': bool,
            'formatted_text': str,  # [MM:SS] í˜•ì‹
            'sentences': list,  # ë¬¸ì¥ë³„ ì •ë³´
            'stats': dict
        }
    """
    processor = SmartSentenceSTT(api_key)
    return processor.process_with_timestamps(
        audio_path, 
        max_workers=max_workers,
        progress_callback=progress_callback
    )