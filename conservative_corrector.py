# conservative_corrector.py
"""
ë³´ìˆ˜ì  STT í…ìŠ¤íŠ¸ ë³´ì • ì‹œìŠ¤í…œ - GPT ìµœì í™” ë²„ì „
Claudeì²˜ëŸ¼ ì‘ë™í•˜ë„ë¡ ê°œì„ 
"""

import os
import json
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from openai import OpenAI


@dataclass
class SuspiciousPart:
    """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„"""
    text: str
    start: int
    end: int
    confidence: float
    reason: str
    suggestions: List[str] = field(default_factory=list)
    linguistic_type: str = ""  # ìŒìš´ì /ì˜ë¯¸ì /ë¬¸ë²•ì 


@dataclass
class CorrectionResult:
    """ë³´ì • ê²°ê³¼"""
    original_text: str
    corrected_text: str
    suspicious_parts: List[SuspiciousPart]
    auto_corrections: List[Dict]
    needs_review: bool
    context_analysis: Dict = field(default_factory=dict)  # ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼


class ConservativeCorrector:
    """ë³´ìˆ˜ì  í…ìŠ¤íŠ¸ ë³´ì •ê¸° - GPT ìµœì í™”"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        
        # ì¼ë°˜ì ì¸ STT ì˜¤ë¥˜ íŒ¨í„´ (í•˜ë“œì½”ë”© ì•„ë‹˜, í•™ìŠµìš©)
        self.common_patterns = self._load_common_patterns()
    
    def _load_common_patterns(self) -> Dict:
        """ì¼ë°˜ì ì¸ STT ì˜¤ë¥˜ íŒ¨í„´ ë¡œë“œ"""
        return {
            "phonetic_confusion": [
                ("ì¤", "ì¡Œ", "ê²Œì„|ë³´ë“œê²Œì„|ì´ê¸°|ì§€ëŠ”"),
                ("ì¼ì •", "ì¼ë“±", "ì„±ì |ë“±ìˆ˜|1ë“±|ì¼ë“±"),
                ("í•œì´", "í•œ ë²ˆë„", "ì—†ì–´ìš”|ì—†ìŠµë‹ˆë‹¤"),
                ("í•œì˜ì–´", "í•œ ë²ˆë„", "ì—†ì–´ìš”|ì—†ìŠµë‹ˆë‹¤"),
                ("ì´ˆì•„", "ì¢‹ì•„", "ì¹œêµ¬|ê¸°ë¶„"),
                ("í”„ì‚¬", "í•™êµ", "ì¹œêµ¬|í•™ìƒ"),
                ("ì†ì´ë¼ê³ ", "ì†í•´ë¼ê³ ", "ê¸°ë¶„|ë‚˜ë¹ "),
            ],
            "spacing_errors": [
                ("ì–´ë•Œìš”", "ì–´ë•Œìš”", None),  # ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¦„
            ]
        }
    
    def create_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - GPTë¥¼ ì–¸ì–´í•™ìë¡œ ë§Œë“¤ê¸°"""
        return """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ í•œêµ­ì–´ STT ì˜¤ë¥˜ ë³´ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìˆ˜ì²œ ê°œì˜ ìŒì„± ì¸ì‹ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•œ ê²½í—˜ì´ ìˆìœ¼ë©°, íŠ¹íˆ í•œêµ­ì–´ ìŒìš´ ë³€í™”ì™€ êµ¬ì–´ì²´ íŠ¹ì„±ì„ ê¹Šì´ ì´í•´í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼:
1. í•œêµ­ì–´ ìŒìš´í•™ - ë°œìŒ ìœ ì‚¬ì„±ìœ¼ë¡œ ì¸í•œ ì˜¤ì¸ì‹ íŒ¨í„´
2. ë¬¸ë§¥ ê¸°ë°˜ ì˜ë¯¸ ë¶„ì„ - ëŒ€í™”ì˜ íë¦„ê³¼ ì£¼ì œ íŒŒì•…
3. êµ¬ì–´ì²´ íŠ¹ì„± - ì¼ìƒ ëŒ€í™”ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„

ì‘ì—… ì›ì¹™:
- í™•ì‹¤í•œ ì˜¤ë¥˜ë§Œ ìˆ˜ì •í•©ë‹ˆë‹¤
- ì• ë§¤í•œ ë¶€ë¶„ì€ ì›ë³¸ì„ ìœ ì§€í•˜ê³  ì˜ì‹¬ êµ¬ê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤
- ê° ìˆ˜ì •ì—ëŠ” ëª…í™•í•œ ì–¸ì–´í•™ì  ê·¼ê±°ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤"""
    
    def create_analysis_prompt(self, text: str) -> str:
        """1ë‹¨ê³„: ë¬¸ë§¥ íŒŒì•… í”„ë¡¬í”„íŠ¸ (Chain of Thought)"""
        return f"""ë‹¤ìŒ STT í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

=== ë¶„ì„í•  í…ìŠ¤íŠ¸ ===
{text}

=== ë‹¨ê³„ë³„ ë¶„ì„ ì§€ì‹œ ===

STEP 1. ëŒ€í™” ì£¼ì œì™€ ìƒí™© íŒŒì•…
- ì´ ëŒ€í™”ì˜ ì£¼ìš” ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€?
- ëŒ€í™” ì°¸ì—¬ìëŠ” ëˆ„êµ¬ì¸ê°€? (ë‚˜ì´, ê´€ê³„ ì¶”ì •)
- ì–´ë–¤ ìƒí™©ì—ì„œ ë‚˜ëˆˆ ëŒ€í™”ì¸ê°€?

STEP 2. ì˜ë¯¸ì  ì¼ê´€ì„± ê²€í† 
- ê° ë¬¸ì¥ì´ ì „ì²´ ë¬¸ë§¥ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
- ì•ë’¤ ë¬¸ì¥ê³¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ê°€?
- ì´ìƒí•˜ê±°ë‚˜ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ ìˆëŠ”ê°€?

STEP 3. ì ì¬ì  STT ì˜¤ë¥˜ ì‹ë³„
- ë¬¸ë§¥ìƒ ì´ìƒí•œ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë‚´ì„¸ìš”
- ê° ì´ìƒí•œ ë¶€ë¶„ì— ëŒ€í•´ "ì™œ ì´ìƒí•œì§€" ì„¤ëª…í•˜ì„¸ìš”
- ìŒìš´ì ìœ¼ë¡œ ìœ ì‚¬í•œ ëŒ€ì•ˆì´ ìˆëŠ”ì§€ ìƒê°í•´ë³´ì„¸ìš”

JSON ì‘ë‹µ:
{{
    "topic": "ëŒ€í™”ì˜ ì£¼ì œ",
    "context_type": "ìƒë‹´|ì¼ìƒëŒ€í™”|êµìœ¡|ì˜ë£Œ|ê¸°íƒ€",
    "participants": {{
        "speaker1": "ì¶”ì • ì •ë³´",
        "speaker2": "ì¶”ì • ì •ë³´"
    }},
    "semantic_issues": [
        {{
            "text": "ì´ìƒí•œ ë¶€ë¶„",
            "reason": "ì™œ ì´ìƒí•œì§€",
            "context_clue": "íŒë‹¨ ê·¼ê±°ê°€ ëœ ë¬¸ë§¥",
            "suggested_correction": "ì œì•ˆí•˜ëŠ” ìˆ˜ì •"
        }}
    ],
    "confidence_level": "high|medium|low"
}}"""
    
    def create_conservative_prompt(self, text: str, context: Optional[Dict] = None) -> str:
        """2ë‹¨ê³„: ë³´ì • í”„ë¡¬í”„íŠ¸ - Few-shot í•™ìŠµ í¬í•¨"""
        
        # Few-shot ì˜ˆì‹œë“¤
        examples = """
=== ë³´ì • ì˜ˆì‹œë“¤ (í•™ìŠµìš©) ===

ì˜ˆì‹œ 1:
ì›ë³¸: "ì–´ì œ ì•„ë¹ ë‘ ë³´ë“œê²Œì„ í•˜ë‹¤ê°€ ì œê°€ ì¤ê±°ë“ ìš”"
ë¬¸ë§¥: ë³´ë“œê²Œì„ì„ í•˜ë©´ì„œ í™”ê°€ ë‚œ ì´ì•¼ê¸°
ë¶„ì„: 'ì¤ê±°ë“ ìš”'ëŠ” ë¬¸ë§¥ìƒ ë¶€ìì—°ìŠ¤ëŸ¬ì›€. ë³´ë“œê²Œì„ + í™”ë‚¬ë‹¤ â†’ 'ì¡Œê±°ë“ ìš”'ê°€ ì ì ˆ
ìˆ˜ì •: "ì–´ì œ ì•„ë¹ ë‘ ë³´ë“œê²Œì„ í•˜ë‹¤ê°€ ì œê°€ ì¡Œê±°ë“ ìš”"
ì‹ ë¢°ë„: 0.9 (ë¬¸ë§¥ì´ ëª…í™•)

ì˜ˆì‹œ 2:
ì›ë³¸: "ì´ˆì•„ ì¹œêµ¬ëŠ” í˜•ì œê°€ ìˆì–´ìš”?"
ë¬¸ë§¥: ìƒë‹´ ëŒ€í™”, ì§ˆë¬¸-ë‹µë³€ í˜•ì‹
ë¶„ì„: 'ì´ˆì•„'ê°€ ë¬¸ì¥ ì‹œì‘ì— ì–´ìƒ‰í•¨. 'ì¢‹ì•„ìš”.' + 'ì¹œêµ¬ëŠ”'ìœ¼ë¡œ ë¶„ë¦¬ ì¶”ì •
ìˆ˜ì •: "ì¢‹ì•„ìš”. ì¹œêµ¬ëŠ” í˜•ì œê°€ ìˆì–´ìš”?"
ì‹ ë¢°ë„: 0.7 (ë¬¸ë§¥ìƒ ì¶”ì •)

ì˜ˆì‹œ 3:
ì›ë³¸: "í•œì´ ì—†ì–´ìš”"
ë¬¸ë§¥: ë¶€ì •ì  ê²½í—˜ ìœ ë¬´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
ë¶„ì„: 'í•œì´'ëŠ” 'í•œ ë²ˆë„'ì˜ ì˜¤ì¸ì‹ìœ¼ë¡œ ì¶”ì •
ìˆ˜ì •: "í•œ ë²ˆë„ ì—†ì–´ìš”"
ì‹ ë¢°ë„: 0.8 (ë¬¸ë²•ì ìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ¬ì›€)

ì˜ˆì‹œ 4:
ì›ë³¸: "ì¼ì •ì„ í•˜ì§€ ëª»í•˜ê²Œ ë ê¹Œë´"
ë¬¸ë§¥: ì„±ì ê³¼ ë“±ìˆ˜ì— ëŒ€í•œ ê±±ì •
ë¶„ì„: 'ì¼ì •'ì€ 'ì¼ë“±'ì˜ ì˜¤ì¸ì‹. ì„±ì  ë¬¸ë§¥ì—ì„œ 'ì¼ë“±'ì´ ì ì ˆ
ìˆ˜ì •: "ì¼ë“±ì„ í•˜ì§€ ëª»í•˜ê²Œ ë ê¹Œë´"
ì‹ ë¢°ë„: 0.85 (ë¬¸ë§¥ì´ ëª…í™•)
"""
        
        context_info = ""
        if context:
            context_info = f"""
=== 1ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ ===
ì£¼ì œ: {context.get('topic', 'ë¶ˆëª…')}
ìƒí™©: {context.get('context_type', 'ì¼ë°˜')}
ì£¼ìš” ë¬¸ì œ: {len(context.get('semantic_issues', []))}ê°œ ë°œê²¬
"""
        
        prompt = f"""{examples}

{context_info}

=== ë³´ì •í•  í…ìŠ¤íŠ¸ ===
{text}

=== ì‘ì—… ì§€ì‹œ ===

ë‹¹ì‹ ì€ ì´ì œ ìœ„ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë³´ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ê° ë¬¸ì¥ì— ëŒ€í•´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

1. ë¬¸ë§¥ í™•ì¸: ì´ ë¬¸ì¥ì´ ì „ì²´ ëŒ€í™”ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
2. ìŒìš´ ê²€í† : ì´ìƒí•œ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ì˜¤ì¸ì‹ì¼ ê°€ëŠ¥ì„±ì€?
3. ì˜ë¯¸ ê²€ì¦: ìˆ˜ì • í›„ ì˜ë¯¸ê°€ ë” ìì—°ìŠ¤ëŸ¬ì›Œì§€ëŠ”ê°€?
4. ì‹ ë¢°ë„ í‰ê°€: ì´ ìˆ˜ì •ì´ ì–¼ë§ˆë‚˜ í™•ì‹¤í•œê°€?

ë³´ì • ì›ì¹™:
- ì‹ ë¢°ë„ 0.7 ì´ìƒë§Œ ìë™ ìˆ˜ì •
- 0.3~0.7ì€ suspicious_partsì— í¬í•¨
- 0.3 ë¯¸ë§Œì€ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ
- ë¬¸ì¥ ëë§ˆë‹¤ ì ì ˆí•œ ë¬¸ì¥ë¶€í˜¸ ì¶”ê°€
- ê° ë¬¸ì¥ ì‚¬ì´ì— ì¤„ë°”ê¿ˆ ì¶”ê°€ (ê°€ë…ì„±)

JSON ì‘ë‹µ í˜•ì‹:
{{
    "corrected_text": "ë³´ì •ëœ í…ìŠ¤íŠ¸ (ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ)",
    "auto_corrections": [
        {{
            "type": "phonetic|semantic|punctuation|spacing",
            "original": "ì›ë³¸",
            "corrected": "ìˆ˜ì •",
            "confidence": 0.0-1.0,
            "reason": "ìˆ˜ì • ì´ìœ ",
            "context_clue": "ê·¼ê±°ê°€ ëœ ë¬¸ë§¥"
        }}
    ],
    "suspicious_parts": [
        {{
            "text": "ì˜ì‹¬ ë¶€ë¶„",
            "start_char": ì‹œì‘,
            "end_char": ë,
            "confidence": 0.3-0.7,
            "reason": "ì˜ì‹¬ ì´ìœ ",
            "linguistic_type": "phonetic|semantic|grammatical",
            "suggestions": ["ëŒ€ì•ˆ1", "ëŒ€ì•ˆ2"],
            "context_clue": "ì£¼ë³€ ë¬¸ë§¥"
        }}
    ],
    "reasoning": "ì „ì²´ì ì¸ ë³´ì • ê·¼ê±°ì™€ ì ‘ê·¼ ë°©ë²•"
}}"""
        
        return prompt
    
    def apply_basic_rules(self, text: str) -> Tuple[str, List[Dict]]:
        """ê¸°ë³¸ ê·œì¹™ ì ìš© (GPT í˜¸ì¶œ ì „)"""
        
        corrections = []
        result = text
        
        # 1. ì¤‘ë³µ ì¡°ì‚¬ ì œê±°
        duplicates = [
            (r'ì„ë¥¼', 'ë¥¼'),
            (r'ì´ê°€', 'ê°€'),
            (r'ì€ëŠ”', 'ëŠ”'),
            (r'ê³¼ì™€', 'ì™€'),
        ]
        
        for pattern, replacement in duplicates:
            if re.search(pattern, result):
                for match in re.finditer(pattern, result):
                    corrections.append({
                        "type": "duplicate",
                        "original": pattern,
                        "corrected": replacement,
                        "confidence": 1.0,
                        "position": match.start()
                    })
                result = re.sub(pattern, replacement, result)
        
        # 2. ë¬¸ì¥ ë ì²˜ë¦¬
        result = re.sub(r'([ê°€-í£])\s*\.\s*([ê°€-í£])', r'\1. \2', result)
        
        # 3. ì—°ì† ê³µë°± ì œê±°
        result = re.sub(r'\s+', ' ', result).strip()
        
        return result, corrections
    
    def post_process_with_patterns(self, text: str, context: Dict) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í›„ì²˜ë¦¬ (ì¼ë°˜í™”ëœ íŒ¨í„´)"""
        
        # ë¬¸ë§¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        topic = context.get('topic', '').lower()
        issues = context.get('semantic_issues', [])
        
        # ë™ì  íŒ¨í„´ ì ìš©
        result = text
        
        # semantic_issuesì—ì„œ ì œì•ˆëœ ìˆ˜ì •ì‚¬í•­ ì ìš©
        for issue in issues:
            if issue.get('suggested_correction'):
                original = issue.get('text', '')
                correction = issue.get('suggested_correction', '')
                if original and correction and original in result:
                    result = result.replace(original, correction)
        
        # ë¬¸ì¥ë³„ ì¤„ë°”ê¿ˆ ì¶”ê°€
        result = re.sub(r'([.!?])\s*(?=[ê°€-í£])', r'\1\n', result)
        
        return result
    
    def correct(self, text: str, use_context_analysis: bool = True) -> CorrectionResult:
        """ë³´ìˆ˜ì  ë³´ì • ì‹¤í–‰ - ê°œì„ ëœ ë²„ì „"""
        
        # ê¸°ë³¸ ê·œì¹™ ì ìš©
        pre_corrected, rule_corrections = self.apply_basic_rules(text)
        
        # 1ë‹¨ê³„: ë¬¸ë§¥ ë¶„ì„ (Chain of Thought)
        context = {}
        if use_context_analysis and len(text) > 50:  # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìŠ¤í‚µ
            print("ğŸ” ë¬¸ë§¥ ë¶„ì„ ì¤‘...")
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an expert in Korean language and STT error analysis."
                        },
                        {"role": "user", "content": self.create_analysis_prompt(pre_corrected)}
                    ],
                    temperature=0.1,  # ë” ë‚®ì¶¤
                    response_format={"type": "json_object"}
                )
                
                context = json.loads(response.choices[0].message.content)
            except Exception as e:
                print(f"âš ï¸ ë¬¸ë§¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
                context = {}
        
        # 2ë‹¨ê³„: GPT ë³´ì • (Few-shot + ë¬¸ë§¥)
        prompt = self.create_conservative_prompt(pre_corrected, context)
        
        try:
            print("âœï¸ ë³´ì • ë¶„ì„ ì¤‘...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": self.create_system_prompt()
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # ì™„ì „ deterministic
                top_p=0.1,  # ê°€ì¥ í™•ì‹¤í•œ ê²ƒë§Œ
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # 3ë‹¨ê³„: í›„ì²˜ë¦¬ (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
            corrected_text = result.get("corrected_text", pre_corrected)
            if context:
                corrected_text = self.post_process_with_patterns(corrected_text, context)
            
            # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ íŒŒì‹±
            suspicious_parts = []
            for susp in result.get("suspicious_parts", []):
                suspicious_parts.append(SuspiciousPart(
                    text=susp["text"],
                    start=susp.get("start_char", 0),
                    end=susp.get("end_char", 0),
                    confidence=susp.get("confidence", 0.5),
                    reason=susp.get("reason", ""),
                    suggestions=susp.get("suggestions", []),
                    linguistic_type=susp.get("linguistic_type", "")
                ))
            
            # ìë™ ìˆ˜ì • ì‚¬í•­ ë³‘í•©
            auto_corrections = rule_corrections + result.get("auto_corrections", [])
            
            return CorrectionResult(
                original_text=text,
                corrected_text=corrected_text,
                suspicious_parts=suspicious_parts,
                auto_corrections=auto_corrections,
                needs_review=len(suspicious_parts) > 0,
                context_analysis=context
            )
            
        except Exception as e:
            print(f"âŒ GPT ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê·œì¹™ë§Œ ì ìš©
            return CorrectionResult(
                original_text=text,
                corrected_text=pre_corrected,
                suspicious_parts=[],
                auto_corrections=rule_corrections,
                needs_review=False,
                context_analysis=context
            )
    
    def correct_in_chunks(self, text: str, chunk_size: int = 10) -> CorrectionResult:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ì„ íƒì )"""
        
        sentences = re.split(r'([.!?]+)', text)
        chunks = []
        current_chunk = []
        
        for i in range(0, len(sentences)-1, 2):
            sentence = sentences[i] + sentences[i+1] if i+1 < len(sentences) else sentences[i]
            current_chunk.append(sentence)
            
            if len(current_chunk) >= chunk_size:
                chunks.append(''.join(current_chunk))
                current_chunk = []
        
        if current_chunk:
            chunks.append(''.join(current_chunk))
        
        # ê° ì²­í¬ ì²˜ë¦¬
        all_corrected = []
        all_suspicious = []
        all_corrections = []
        
        for chunk in chunks:
            result = self.correct(chunk, use_context_analysis=True)
            all_corrected.append(result.corrected_text)
            all_suspicious.extend(result.suspicious_parts)
            all_corrections.extend(result.auto_corrections)
        
        # ë³‘í•©
        final_text = '\n'.join(all_corrected)
        
        return CorrectionResult(
            original_text=text,
            corrected_text=final_text,
            suspicious_parts=all_suspicious,
            auto_corrections=all_corrections,
            needs_review=len(all_suspicious) > 0,
            context_analysis={}
        )


class InteractiveReviewer:
    """ì‚¬ìš©ì ê²€í†  ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self):
        self.review_history = []
    
    def display_for_review(self, result: CorrectionResult) -> Dict:
        """ê²€í† ìš© í‘œì‹œ ë°ì´í„° ìƒì„±"""
        
        display_data = {
            "text": result.corrected_text,
            "highlights": [],
            "review_needed": result.needs_review,
            "context": result.context_analysis
        }
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ í•˜ì´ë¼ì´íŠ¸ ì •ë³´
        for susp in result.suspicious_parts:
            highlight = {
                "start": susp.start,
                "end": susp.end,
                "text": susp.text,
                "reason": susp.reason,
                "linguistic_type": susp.linguistic_type,
                "suggestions": susp.suggestions,
                "confidence": susp.confidence
            }
            
            # ì–¸ì–´í•™ì  íƒ€ì…ë³„ ìƒ‰ìƒ
            if susp.linguistic_type == "phonetic":
                highlight["color"] = "#FF6B6B"  # ë¹¨ê°• - ìŒìš´ì 
            elif susp.linguistic_type == "semantic":
                highlight["color"] = "#4ECDC4"  # ì²­ë¡ - ì˜ë¯¸ì 
            elif susp.linguistic_type == "grammatical":
                highlight["color"] = "#45B7D1"  # íŒŒë‘ - ë¬¸ë²•ì 
            else:
                highlight["color"] = "#FFA07A"  # ì£¼í™© - ê¸°íƒ€
            
            display_data["highlights"].append(highlight)
        
        # ìë™ ìˆ˜ì • ì‚¬í•­ ìš”ì•½
        display_data["auto_corrections_summary"] = self._summarize_corrections(
            result.auto_corrections
        )
        
        return display_data
    
    def _summarize_corrections(self, corrections: List[Dict]) -> str:
        """ìë™ ìˆ˜ì • ì‚¬í•­ ìš”ì•½"""
        
        if not corrections:
            return "ìë™ ìˆ˜ì • ì—†ìŒ"
        
        summary = []
        types = {}
        for corr in corrections:
            corr_type = corr.get("type", "ê¸°íƒ€")
            types[corr_type] = types.get(corr_type, 0) + 1
        
        for corr_type, count in types.items():
            if corr_type == "punctuation":
                summary.append(f"ë¬¸ì¥ë¶€í˜¸ {count}ê°œ")
            elif corr_type == "duplicate":
                summary.append(f"ì¤‘ë³µ ì¡°ì‚¬ {count}ê°œ")
            elif corr_type == "spacing":
                summary.append(f"ë„ì–´ì“°ê¸° {count}ê°œ")
            elif corr_type == "phonetic":
                summary.append(f"ìŒìš´ ì˜¤ë¥˜ {count}ê°œ")
            elif corr_type == "semantic":
                summary.append(f"ì˜ë¯¸ ì˜¤ë¥˜ {count}ê°œ")
            else:
                summary.append(f"{corr_type} {count}ê°œ")
        
        return ", ".join(summary)
    
    def apply_user_corrections(self, result: CorrectionResult, 
                              user_choices: Dict[str, str]) -> str:
        """ì‚¬ìš©ì ì„ íƒ ì ìš©"""
        
        final_text = result.corrected_text
        
        # ì‚¬ìš©ì ì„ íƒ ì ìš© (ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¸ë±ìŠ¤ ê¼¬ì„ ë°©ì§€)
        replacements = []
        for original, replacement in user_choices.items():
            # í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì°¾ê¸°
            idx = final_text.find(original)
            if idx >= 0:
                replacements.append((idx, original, replacement))
        
        # ìœ„ì¹˜ ì—­ìˆœ ì •ë ¬
        replacements.sort(reverse=True)
        
        # ì¹˜í™˜ ì ìš©
        for idx, original, replacement in replacements:
            final_text = final_text[:idx] + replacement + final_text[idx + len(original):]
        
        # ê¸°ë¡ ì €ì¥
        self.review_history.append({
            "original": result.original_text,
            "auto_corrected": result.corrected_text,
            "user_choices": user_choices,
            "final": final_text,
            "context": result.context_analysis
        })
        
        return final_text


# í¸ì˜ í•¨ìˆ˜
def conservative_correct_with_review(text: str, api_key: Optional[str] = None,
                                    use_context: bool = True,
                                    use_chunks: bool = False) -> Dict:
    """
    ë³´ìˆ˜ì  ë³´ì • + ê²€í†  í•„ìš” ì—¬ë¶€ ë°˜í™˜
    
    Args:
        text: STT ì¶œë ¥ í…ìŠ¤íŠ¸
        api_key: OpenAI API í‚¤
        use_context: ë¬¸ë§¥ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€
        use_chunks: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì—¬ë¶€
    
    Returns:
        {
            "corrected": ë³´ì •ëœ í…ìŠ¤íŠ¸,
            "needs_review": ê²€í†  í•„ìš” ì—¬ë¶€,
            "review_items": ê²€í†  í•­ëª©ë“¤,
            "summary": ìš”ì•½,
            "context": ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼
        }
    """
    
    corrector = ConservativeCorrector(api_key)
    reviewer = InteractiveReviewer()
    
    # ë³´ì • ì‹¤í–‰
    if use_chunks and len(text) > 500:  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ
        result = corrector.correct_in_chunks(text)
    else:
        result = corrector.correct(text, use_context_analysis=use_context)
    
    # ê²€í† ìš© ë°ì´í„° ìƒì„±
    display_data = reviewer.display_for_review(result)
    
    return {
        "corrected": result.corrected_text,
        "needs_review": result.needs_review,
        "review_items": display_data["highlights"],
        "auto_corrections": display_data["auto_corrections_summary"],
        "original": text,
        "context": display_data.get("context", {})
    }


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # ì‹¤ì œ ì˜¤ë¥˜ê°€ ìˆëŠ” í…ìŠ¤íŠ¸
    test_text = """ì–´ì œ ì•„ë¹ ë‘ ë³´ë“œê²Œì„ í•˜ë‹¤ê°€ ì œê°€ ì¤ê±°ë“ ìš”. 
    ì œê°€ ì›ë˜ ì§€ëŠ” ê±¸ ë„ˆë¬´ ì‹«ì–´í•´ì„œìš”.
    ì¼ì •ì„ í•˜ì§€ ëª»í•˜ê²Œ ë ê¹Œ ë´ ë¬´ì„œì›Œìš”.
    ì´ˆì•„ ì¹œêµ¬ëŠ” í˜•ì œê°€ ìˆì–´ìš”?
    í”„ì‚¬ ì¹œêµ¬ë“¤ì´ë‘ì€ ì£¼ë¡œ ë­í•˜ê³  ë†€ì•„ìš”?
    í•œì´ ì—†ì–´ìš”."""
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
    else:
        print("="*60)
        print("ğŸ” ë³´ìˆ˜ì  ë³´ì • + ì‚¬ìš©ì ê²€í†  ì‹œìŠ¤í…œ")
        print("="*60)
        
        # ë³´ì • ì‹¤í–‰
        result = conservative_correct_with_review(test_text, api_key, use_context=True)
        
        print(f"\nğŸ“ ì›ë³¸:")
        print(result["original"])
        
        print(f"\nâœï¸ ìë™ ë³´ì •:")
        print(result["corrected"])
        print(f"ìë™ ìˆ˜ì •: {result['auto_corrections']}")
        
        if result.get("context"):
            print(f"\nğŸ“Š ë¬¸ë§¥ ë¶„ì„:")
            print(f"ì£¼ì œ: {result['context'].get('topic', 'ë¶ˆëª…')}")
            print(f"ìœ í˜•: {result['context'].get('context_type', 'ì¼ë°˜')}")
        
        if result["needs_review"]:
            print(f"\nâš ï¸ ê²€í†  í•„ìš”: {len(result['review_items'])}ê°œ í•­ëª©")
            for i, item in enumerate(result["review_items"], 1):
                print(f"\n  {i}. [{item['text']}]")
                print(f"     ìœ í˜•: {item.get('linguistic_type', 'ì¼ë°˜')}")
                print(f"     ì´ìœ : {item['reason']}")
                print(f"     ì œì•ˆ: {item['suggestions']}")
                print(f"     ì‹ ë¢°ë„: {item['confidence']:.0%}")
        else:
            print("\nâœ… ê²€í†  í•„ìš” ì—†ìŒ")